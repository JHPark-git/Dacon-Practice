{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dacon 한국어 문장 관계 분석 경진대회 연습.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1nP_riWVkb0mZUjfYfNBUOIOpFUF3V-n1",
      "authorship_tag": "ABX9TyNhuYLe0HTsvVXdF1ASsUvd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JHPark-git/Repo/blob/main/Dacon_%ED%95%9C%EA%B5%AD%EC%96%B4_%EB%AC%B8%EC%9E%A5_%EA%B4%80%EA%B3%84_%EB%B6%84%EC%84%9D_%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C_%EC%97%B0%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [NLP] NLI/한국어 문장 관계 분석 경진대회 연습 \n",
        "[휘오/Using Roberta Large](https://dacon.io/competitions/official/235875/codeshare/4597?page=1&dtype=recent), [yuntaeyang/Roberta Large 베이스라인](https://dacon.io/competitions/official/235875/codeshare/4591?page=3&dtype=recent)를 참고해 공부함."
      ],
      "metadata": {
        "id": "vVJkfOHLnK2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLI는 두 문장 간의 관계를 entailment, neutral, contradiction의 3 label로 분류하는 작업으로 NLU의 기반."
      ],
      "metadata": {
        "id": "_UHjntyalqKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data**\n",
        "\n",
        "[klue_nli_train](https://klue-benchmark.com/tasks/68/data/description) 24998 set을 train data로, [klue_nli_dev](https://klue-benchmark.com/tasks/68/data/description) 3000 set의 hypothesis를 가공해 제공.\n",
        "\n",
        "이에 klue_nli_dev data를 train data에 추가해 학습.\n",
        "\n",
        "NSMC(네이버 영화 감정 분석). airbnb review, wikinews, wikipedia, wikitree, policy로 구성.\n",
        "\n",
        "NSMC, airbnb review/wikinews, wikipedia, wikitree, policy 두 그룹으로 분류 가능.\n",
        "\n",
        "두 그룹의 평균 문장 길이, 자주 사용되는 단어, 문장 구성, 문맥 등의 차이점이 존재할 것으로 예상.\n",
        "\n",
        "추후 그룹 feature를 추가해 학습 모델을 구성해 볼 예정."
      ],
      "metadata": {
        "id": "4elfz9KCpZbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train**\n",
        "\n",
        "Roberta-Large 사전학습 모델\n",
        "\n",
        "모델1: 5fold, 5epochs\n",
        "\n",
        "모델2: 5fold, 10epochs\n",
        "\n",
        "모델3: 10fold, 5epochs\n",
        "\n",
        "모델4: 10fold, 10epochs\n",
        "\n",
        "4개 모델 seed를 다르게 해 학습후, soft voting으로 ensemble."
      ],
      "metadata": {
        "id": "vtjM0KtLttie"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUQMPutEx1Rf",
        "outputId": "6fc6e24d-745b-4191-8ca4-91e1535b24a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 30.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 38.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 19.6 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 36.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 37.4 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 13.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 33.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 38.2 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 38.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, pyyaml, fsspec, aiohttp, xxhash, tokenizers, sacremoses, responses, huggingface-hub, transformers, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.2.0 huggingface-hub-0.4.0 multidict-6.0.2 pyyaml-6.0 responses-0.18.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd \n",
        "import random\n",
        "import numpy as np \n",
        "import os\n",
        "import re\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tqdm import tqdm\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AdamW, RobertaForSequenceClassification\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ],
      "metadata": {
        "id": "cJAh_AjnyK-o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_zip_file = '/content/drive/MyDrive/Colab Notebooks/dataset/한국어문장관계분류.zip'\n",
        "directory_to_extract_to = '/content'\n",
        "\n",
        "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directory_to_extract_to)"
      ],
      "metadata": {
        "id": "manrliP3yG1L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**!tar -xzvf**\n",
        "\n",
        "[klue_dataset](https://klue-benchmark.com/tasks/68/data/description)에서 다운로드한 tar.gz file 압축해제"
      ],
      "metadata": {
        "id": "cevrB6aSvYXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf '/content/drive/MyDrive/Colab Notebooks/dataset/klue-nli-v1.1.tar.gz'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jdk8VQV4Y77",
        "outputId": "eb21e7bd-3b39-45dd-98fc-17c1644f18d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "klue-nli-v1.1/\n",
            "klue-nli-v1.1/klue-nli-v1.1_dev.json\n",
            "klue-nli-v1.1/klue-nli-v1.1_dev_sample_10.json\n",
            "klue-nli-v1.1/klue-nli-v1.1_train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pytorch 실험 결과 재현을 위한 시드 설정\n",
        "\n",
        "**random.seed(seed)**: python random lib\n",
        "\n",
        "**np.random.seed(seed)**: numpy lib\n",
        "\n",
        "**os.environ['PYTHONHASHSEED'] = str(seed)**: python hashing algorithm\n",
        "\n",
        "**torch.manual_seed(seed)**: pytorch\n",
        "\n",
        "**torch.cuda.manual_seed/seed_all(seed)**: cuda/multi gpu\n",
        "\n",
        "**torch.backends.cudnn.deterministic/benchmark = True**: cuda lib cudnn"
      ],
      "metadata": {
        "id": "n3BGI7riv2S1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=42):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED']=str(seed)\n",
        "  torch.manual_seed(seed) # pytorch random seed 설정\n",
        "  torch.cuda.manual_seed(seed) \n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic = True #cuda random seed 설정/연산 처리 속도 감소 부작용>마지막 검증단계에 사용\n",
        "  torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "Imzs1XCzyYQx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**전처리**\n",
        "\n",
        "train_dataset concat 위해 premise, hypothesis, label 항만 추출\n",
        "\n",
        "**map(dict)**: label to index\n",
        "\n",
        "**pd.drop_duplicates()**: 중복항 제거\n",
        "\n"
      ],
      "metadata": {
        "id": "oJf08DKo2UeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(train='train_data.csv', test='test_data.csv'):\n",
        "  train = pd.read_csv('/content/open/'+ train)\n",
        "  test = pd.read_csv('/content/open/'+ test)\n",
        "  submission = pd.read_csv('/content/open/sample_submission.csv')\n",
        "  train_new = pd.read_json('/content/klue-nli-v1.1/klue-nli-v1.1_dev.json')\n",
        "  train_new= train_new[['premise','hypothesis','gold_label']]\n",
        "  train_new.columns = ['premise','hypothesis','label']\n",
        "  train_new = train_new.reset_index()\n",
        "  train = pd.concat([train, train_new],ignore_index=True)\n",
        "  label_dict = {'entailment':0,'neutral':1,'contradiction':2}\n",
        "  train['label'] = train['label'].map(label_dict)\n",
        "  train.drop_duplicates(inplace=True)\n",
        "  train = train[['premise','hypothesis','label']]\n",
        "  test = test[['premise','hypothesis']]\n",
        "  return train, test, submission"
      ],
      "metadata": {
        "id": "oolDgSF-zcXm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test, submission = load_data()"
      ],
      "metadata": {
        "id": "CoHPXaU0bGoU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델1"
      ],
      "metadata": {
        "id": "mBrPyysrUfxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')\n",
        "epochs = 5\n",
        "batch_size = 8\n",
        "lr = 1e-4\n",
        "n_split = 5"
      ],
      "metadata": {
        "id": "20L3eP2Jlyjc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델2"
      ],
      "metadata": {
        "id": "APwAeURpUliW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(seed=2002)\n",
        "device = torch.device('cuda')\n",
        "epochs = 5\n",
        "batch_size = 8\n",
        "lr = 1e-4\n",
        "n_split = 10"
      ],
      "metadata": {
        "id": "04rtDBc8nb0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델3"
      ],
      "metadata": {
        "id": "PptXFJPnTIpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(seed=3)\n",
        "device = torch.device('cuda')\n",
        "epochs = 10\n",
        "batch_size = 8\n",
        "lr = 1e-4\n",
        "n_split = 5"
      ],
      "metadata": {
        "id": "tWP-1CKPTFvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델4"
      ],
      "metadata": {
        "id": "J5u3nU2vTKBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(seed=50)\n",
        "device = torch.device('cuda')\n",
        "epochs = 10\n",
        "batch_size = 8\n",
        "lr = 1e-4\n",
        "n_split = 10"
      ],
      "metadata": {
        "id": "DPX6KKP6TISU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AutoTokenizer.for_pretrained('klue/roberta-large')(sentences, max_length, pad_to_max_length, add_special_tokens, truncation)**\n",
        "\n",
        "사전훈련 tokenizer로 입력받은 sentences를 토큰화.\n",
        "\n",
        "**max_length**: 토큰화 개수\n",
        "\n",
        "**add_special_tokens**: 시작 토큰[CLS], 문장 맺음 토큰[SEP] 등 추가\n",
        "\n",
        "**truncation**: max_length보다 문장 토큰의 개수가 많을 시, 절삭 기준\n",
        "\n",
        "BERT 모델과 달리 Roberta 모델은 token_type_ids 인자 불요"
      ],
      "metadata": {
        "id": "LMlrlxnI3c_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, dataset, option):\n",
        "    self.dataset = dataset\n",
        "    self.option = option\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    row = self.dataset.iloc[idx, 0:3].values\n",
        "    pre = row[0]\n",
        "    hyp = row[1]\n",
        "    inputs = self.tokenizer(\n",
        "        pre,\n",
        "        hyp,\n",
        "        return_tensors='pt',\n",
        "        max_length = 100,\n",
        "        pad_to_max_length = True,\n",
        "        add_special_tokens = True,\n",
        "        truncation = True)\n",
        "    \n",
        "    input_ids = inputs['input_ids'][0]\n",
        "    attention_mask = inputs['attention_mask'][0]\n",
        "\n",
        "    if self.option =='train':\n",
        "      label = row[2]\n",
        "      return input_ids, attention_mask, label\n",
        "\n",
        "    return input_ids, attention_mask"
      ],
      "metadata": {
        "id": "sV70FS6DnuWV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "평가척도로서 accuracy 함수 설정\n",
        "\n",
        "**torch.max(tensor, dim)**\n",
        "\n",
        "dim 기준으로 tensor의 최대값 출력, 출력 format: (max_values, max_indices) tuple\n",
        "\n",
        "train_acc: 예측이 맞은 항의 개수/전체 항"
      ],
      "metadata": {
        "id": "SGlhq6mA5pS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(preds,labels):\n",
        "  max_values, max_indices = torch.max(preds,1)\n",
        "  train_acc = (max_indices == labels).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "  return train_acc"
      ],
      "metadata": {
        "id": "hvW3QbKf61yt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[RobertaForSequenceClassification](https://kyunghyunlim.github.io/ml_ai/2021/10/04/roberta_cls.html)\n",
        "\n",
        "**RobertaForSequenceClassification.from_pretrained('klue/roberta-large',num_labels)**\n",
        "\n",
        "model(token_ids, attention_masks)의 output으로 각 클래스의 logits값을 출력\n",
        "\n",
        "[transformers optimization](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup)\n",
        "\n",
        "**Optimizer**: transformers.AdamW(parameters, lr, weight_decay, correct_bias=False)\n",
        "=BERTAdam\n",
        "\n",
        "**Scheduler**: transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
        "\n",
        "0-초기 설정 lr 사이에서 작동하는 cosine함수에 따라 감소하는 lr scheduler\n",
        "\n",
        "warmup_steps: 작동하는 스텝 간격\n",
        "\n",
        "batch마다 scheduler.step()"
      ],
      "metadata": {
        "id": "pN-xUr-38AbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training(train_dataset, val_dataset, fold):\n",
        "  best_acc=0\n",
        "  model = RobertaForSequenceClassification.from_pretrained('klue/roberta-large',num_labels=3).to(device)\n",
        "  dataset_train = CustomDataset(train_dataset,option='train')\n",
        "  dataset_val = CustomDataset(val_dataset,option='train')\n",
        "  train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "  valid_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
        "  optimizer = AdamW(model.parameters(), lr = lr, weight_decay=1e-2, correct_bias=False)\n",
        "  total_steps = len(train_loader) * epochs\n",
        "  scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
        "                                              num_warmup_steps = 1,\n",
        "                                              num_training_steps=total_steps)\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    train_acc = 0.0\n",
        "    valid_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id,(token_ids, attention_masks, labels)in tqdm(enumerate(train_loader)):\n",
        "      optimizer.zero_grad()\n",
        "      token_ids = token_ids.to(device)\n",
        "      attention_masks = attention_masks.to(device)\n",
        "      labels = labels.to(device)\n",
        "      preds = model(token_ids, attention_masks)[0]\n",
        "      loss = F.cross_entropy(preds, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      train_acc += calc_accuracy(preds, labels)\n",
        "\n",
        "    print('epoch {} train acc {}'.format(epoch+1, train_acc/(batch_id+1)))\n",
        "    \n",
        "    model.eval\n",
        "    for batch_id, (token_ids, attention_masks, labels) in tqdm(enumerate(valid_loader)):\n",
        "      token_ids = token_ids.to(device)\n",
        "      attention_masks = attention_masks.to(device)\n",
        "      labels = labels.to(device)\n",
        "      preds = model(token_ids, attention_masks)[0]\n",
        "      valid_acc += calc_accuracy(preds, labels)\n",
        "    print('epoch {} valid acc {}'.format(epoch+1, valid_acc/(batch_id+1)))\n",
        "    torch.save(model, '/content/drive/MyDrive/Colab Notebooks/model'+str(fold)+'fold'+str(epochs)+'epochs'+'.pt')"
      ],
      "metadata": {
        "id": "-pgb90KY7Wsd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cuda 사용시, memory 부족 문제로 인한 캐시 정리 함수"
      ],
      "metadata": {
        "id": "udo-ZziqDhrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "kQ5CQZZzf-sn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**StratifiedKFold(n_splits).split(x,y)**\n",
        "\n",
        "y를 기준으로 x fold.\n",
        "\n",
        "pd.DataFrame.iloc vs pd.DataFrame.loc\n",
        "\n",
        "integer location(index) vs location(label)\n",
        "\n"
      ],
      "metadata": {
        "id": "9GViopc0D3x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folds = []\n",
        "skf = StratifiedKFold(n_splits=n_split, shuffle=True, random_state=42)\n",
        "for train_idx, val_idx in skf.split(train.iloc[:,0:2],train.iloc[:,2]):\n",
        "  folds.append((train.iloc[train_idx,:], train.iloc[val_idx,:]))\n",
        "\n",
        "for fold, (train_dataset, valid_dataset) in enumerate(folds):\n",
        "  print(f'fold{fold} 학습중...')\n",
        "  training(train_dataset = train_dataset, val_dataset=valid_dataset, fold=fold)"
      ],
      "metadata": {
        "id": "v4U61XADDxnR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "outputId": "6b2d89cb-1617-4ecd-cc70-cb10404f8579"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold0 학습중...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2800it [52:29,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 train acc 0.3350297619047619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "700it [04:20,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 valid acc 0.3417857142857143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2800it [52:29,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 train acc 0.33623511904761905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "700it [04:20,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 valid acc 0.3335714285714286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2800it [52:29,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3 train acc 0.3385119047619048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "700it [04:20,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3 valid acc 0.34160714285714283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "265it [04:58,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6271ac1c0f18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'fold{fold} 학습중...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-64d385a417c0>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(train_dataset, val_dataset, fold)\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, dataset_test):\n",
        "  test_dataset = CustomDataset(test, option='test')\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "  model.eval()\n",
        "  output_prob = []\n",
        "  with torch.no_grad():\n",
        "    for batch_id, (token_ids, attention_masks) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
        "      token_ids = token_ids.long().to(device)\n",
        "      attention_masks = attention_masks.long().to(device)\n",
        "      output = model(token_ids, attention_masks)[0]\n",
        "      logits = torch.nn.functional.softmax(output, dim=1).detach().cpu().numpy()\n",
        "      output_prob.extend(logits)\n",
        "  return output_prob"
      ],
      "metadata": {
        "id": "1EkvavZ9setU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = {'entailment':0, 'neutral':1, 'contradiction':2}"
      ],
      "metadata": {
        "id": "yxeqOF2G0JIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_exe(folds, epochs):\n",
        "  res = np.zeros((len(test),3))\n",
        "  for fold in range(folds):\n",
        "    print(f'fold{fold}'+ f'epochs{epochs} 모델 추론중...')\n",
        "    model = torch.load('/content/drive/MyDrive/Colab Notebooks/model'+str(fold)+'fold'+str(epochs)+'epochs'+'.pt')\n",
        "    pred_prob = inference(model, test)\n",
        "    res += np.array(pred_prob)/folds\n",
        "  return res"
      ],
      "metadata": {
        "id": "vSy1ag3x0Z86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ch5_epochs5   = inference_exe(folds=5, epochs=5)\n",
        "ch5_epochs10  = inference_exe(folds=5, epochs=10)\n",
        "ch10_epochs5  = inference_exe(folds=10, epochs=5)\n",
        "ch10_epochs10 = inference_exe(folds=10, epochs=10)\n",
        "\n",
        "ensembled_prob = (ch5_epochs5 + ch5_epochs10 + ch10_epochs5 + ch10_epochs10)/4\n",
        "ans = np.argmax(ensembled_prob, axis=1)\n",
        "out = [list(label_dict.keys())[_] for _ in ans]\n",
        "submission['label'] = out\n",
        "\n",
        "#out = pd.Series(np.argmax(ans,axis=1)).apply(lambda x:(v:k for k,v in label_dict.items())[x])"
      ],
      "metadata": {
        "id": "xhLAzZ1JQUe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv('/content/drive/MyDrive/Colab Notebooks/submission/ensemble_1.csv',index=False)"
      ],
      "metadata": {
        "id": "FDQ_25CUbSWG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}